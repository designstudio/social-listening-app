import streamlit as st
import pandas as pd
import json
import io
from docx import Document
from youtube_comment_downloader import YoutubeCommentDownloader
import re
import matplotlib.pyplot as plt
import seaborn as sns
import networkx as nx
from wordcloud import WordCloud
import os
import google.generativeai as genai

# --- CONFIGURA√á√ÉO GEMINI ---
gemini_api_key = st.secrets.get("GOOGLE_API_KEY")
if not gemini_api_key:
    st.error("A chave da API do Google Gemini n√£o foi encontrada. Configure-a no 'secrets.toml' ou como vari√°vel de ambiente GOOGLE_API_KEY.")
    st.stop()
genai.configure(api_key=gemini_api_key)
model = genai.GenerativeeraGenerativeModel('gemini-2.0-flash')

# --- CONSTANTE PARA LIMITE DE COMENT√ÅRIOS ---
MAX_COMMENTS_TO_PROCESS = 100 # Limite de 100 coment√°rios

# --- EXTRA√á√ÉO DE DADOS ---
@st.cache_data(show_spinner=False)
def extract_text_from_file(file_contents, file_extension):
    text_content_list = []
    try:
        if file_extension == '.csv':
            df = pd.read_csv(io.StringIO(file_contents.decode('utf-8')))
            if 'comentario' in df.columns:
                text_content_list = df['comentario'].dropna().astype(str).tolist()
            else:
                for _, row in df.iterrows():
                    text_content_list.append(" ".join(row.dropna().astype(str).tolist()))
        elif file_extension in ['.xls', '.xlsx']:
            df = pd.read_excel(io.BytesIO(file_contents))
            if 'comentario' in df.columns:
                text_content_list = df['comentario'].dropna().astype(str).tolist()
            else:
                for _, row in df.iterrows():
                    text_content_list.append(" ".join(row.dropna().astype(str).tolist()))
        elif file_extension in ['.doc', '.docx']:
            document = Document(io.BytesIO(file_contents))
            for para in document.paragraphs:
                if para.text.strip():
                    text_content_list.append(para.text)
        else:
            st.warning(f"Formato de arquivo n√£o suportado: {file_extension}.")
            return []
    except Exception as e:
        st.error(f"Erro ao extrair texto do arquivo: {e}")
        return []
    
    return text_content_list

@st.cache_data(show_spinner=False)
def download_youtube_comments(youtube_url):
    try:
        downloader = YoutubeCommentDownloader()
        video_id = None
        match = re.search(r'(?:v=|/)([0-9A-Za-z_-]{11}).*', youtube_url)
        if match:
            video_id = match.group(1)
        if not video_id:
            st.error(f"URL do YouTube inv√°lida: {youtube_url}.")
            return []
        
        with st.spinner(f"Baixando coment√°rios do v√≠deo: {youtube_url}..."):
            all_comments = []
            try:
                comments_generator = downloader.get_comments(video_id)
                for comment in comments_generator:
                    all_comments.append(comment['text'])
                    # Adicione um limite interno para o downloader do YouTube para evitar downloads excessivos
                    if len(all_comments) >= MAX_COMMENTS_TO_PROCESS * 2: # Baixa um pouco mais para ter certeza de atingir o limite
                        break
            except Exception as e:
                st.error(f"N√£o foi poss√≠vel baixar coment√°rios: {e}")
                return []
            
            return all_comments

    except Exception as e:
        st.error(f"Erro geral ao baixar coment√°rios: {e}.")
        return []

# --- AN√ÅLISE COM GEMINI ---
@st.cache_data(show_spinner=True)
def analyze_text_with_gemini(text_to_analyze):
    if not text_to_analyze.strip():
        return {
            "sentiment": {"positive": 0.0, "neutral": 0.0, "negative": 0.0, "no_sentiment_detected": 100.0},
            "topics": [],
            "term_clusters": {},
            "topic_relations": []
        }
    
    num_comments_in_prompt = len(text_to_analyze.split('\n'))
    
    prompt = f"""
Analise o texto de coment√°rios de redes sociais abaixo de forma estritamente objetiva, factual e consistente.
Estes {num_comments_in_prompt} coment√°rios s√£o uma amostra ou o total de coment√°rios dispon√≠veis para an√°lise.
Extraia as informa√ß√µes solicitadas. Calcule as porcentagens e contagens EXATAS com base no total de coment√°rios relevantes.
Forne√ßa as seguintes informa√ß√µes em formato JSON, exatamente como a estrutura definida. N√£o inclua nenhum texto adicional antes ou depois do JSON.
1. Sentimento Geral: A porcentagem de coment√°rios classificados como 'Positivo', 'Neutro', 'Negativo' e 'Sem Sentimento Detectado'. As porcentagens devem somar 100%.
2. Temas Mais Citados: Lista de 5 a 10 temas principais discutidos nos coment√°rios, com contagem EXATA de coment√°rios Positivos, Neutros e Negativos.
3. Agrupamento de Termos/Nuvem de Palavras: Lista dos 10 a 20 termos ou palavras-chave mais frequentes. Para cada termo, forne√ßa a frequ√™ncia (contagem de ocorr√™ncias).
4. Rela√ß√£o entre Temas: Liste 3 a 5 pares de temas que frequentemente aparecem juntos, indicando rela√ß√£o clara e l√≥gica.
O JSON deve ter a seguinte estrutura:
{{
  "sentiment": {{
    "positive": float,
    "neutral": float,
    "negative": float,
    "no_sentiment_detected": float
  }},
  "topics": [
    {{
      "name": "Nome do Tema",
      "positive": int,
      "neutral": int,
      "negative": int
    }}
  ],
  "term_clusters": {{
    "termo1": int,
    "termo2": int
  }},
  "topic_relations": [
    {{
      "source": "Tema A",
      "target": "Tema B",
      "description": "Breve descri√ß√£o da rela√ß√£o"
    }}
  ]
}}
Texto para an√°lise:
"{text_to_analyze}"
"""
    try:
        response = model.generate_content(prompt)
        response_text = response.text.strip()
        if response_text.startswith("```json"):
            response_text = response_text[len("```json"):].strip()
        if response_text.endswith("```"):
            response_text = response_text[:-len("```")].strip()
        data = json.loads(response_text)
        
        if 'no_sentiment_detected' not in data['sentiment']:
            total = data['sentiment'].get('positive', 0) + data['sentiment'].get('neutral', 0) + data['sentiment'].get('negative', 0)
            data['sentiment']['no_sentiment_detected'] = round(100.0 - total, 2)
        
        total_sum = sum(data['sentiment'].values())
        if total_sum != 100 and total_sum != 0:
            for key in data['sentiment']:
                data['sentiment'][key] = round(data['sentiment'][key] / total_sum * 100, 2)
        return data
    except json.JSONDecodeError as e:
        st.error(f"Erro ao decodificar JSON da resposta do Gemini: {e}")
        st.code(f"Resposta bruta do Gemini (An√°lise): {response_text}")
        return None
    except Exception as e:
        st.error(f"Erro inesperado ao analisar com Gemini: {e}")
        return None

@st.cache_data(show_spinner=True)
def generate_qualitative_analysis(analysis_results, original_text_sample):
    sentiment = analysis_results.get('sentiment', {})
    topics = analysis_results.get('topics', [])
    term_clusters = analysis_results.get('term_clusters', {})
    topic_relations = analysis_results.get('topic_relations', [])
    prompt = f"""
Com base na an√°lise de social listening dos coment√°rios, atue como um especialista em Marketing de Produto, Social Listening e Data Analysis. Redija uma an√°lise qualitativa abrangente em at√© 4 par√°grafos, focando nos aprendizados e insights estrat√©gicos.
Considere as seguintes informa√ß√µes:
Sentimento Geral: {json.dumps(sentiment, ensure_ascii=False)}
Temas Mais Citados: {json.dumps(topics, ensure_ascii=False)}
Agrupamento de Termos: {json.dumps(term_clusters, ensure_ascii=False)}
Rela√ß√£o entre Temas: {json.dumps(topic_relations, ensure_ascii=False)}
"""
    try:
        response = model.generate_content(prompt)
        return response.text.strip()
    except Exception as e:
        st.error(f"Erro na an√°lise qualitativa: {e}")
        return "N√£o foi poss√≠vel gerar a an√°lise qualitativa."

@st.cache_data(show_spinner=True)
def generate_persona_insights(analysis_results, original_text_sample):
    sentiment = analysis_results.get('sentiment', {})
    topics = analysis_results.get('topics', [])
    term_clusters = analysis_results.get('term_clusters', {})
    original_text_display = original_text_sample[:1000] + "..." if len(original_text_sample) > 1000 else original_text_sample
    combined_context = f"""
Coment√°rios originais (amostra): {original_text_display}
Resultados da an√°lise:
- Sentimento Geral: {json.dumps(sentiment, ensure_ascii=False)}
- Temas Mais Citados: {json.dumps(topics, ensure_ascii=False)}
- Agrupamento de Termos: {json.dumps(term_clusters, ensure_ascii=False)}
"""
    persona_prompt = f"""
Considerando a an√°lise de social listening de um p√∫blico e o conceito de "personas sint√©ticas" (personas criadas a partir de dados comportamentais e sentimentos reais), crie um insight de persona sint√©tica para o p√∫blico desses coment√°rios.
Descreva:
- As principais dores e necessidades desse p√∫blico;
- Seus interesses ou paix√µes (expl√≠citas ou impl√≠citas);
- O tom predominante da comunica√ß√£o (positivo, negativo, neutro, impaciente, engajado, etc);
- Oportunidades para engajar ou atender melhor essa persona;
- D√™ um nome sugestivo √† persona, como "O Cr√≠tico Construtivo" ou "A Consumidora Engajada".
Use at√© 3-4 par√°grafos.  
Contexto da an√°lise:
{combined_context}
"""
    try:
        response = model.generate_content(persona_prompt)
        return response.text.strip()
    except Exception as e:
        st.error(f"Erro ao gerar insights para persona sint√©tica: {e}")
        return "N√£o foi poss√≠vel gerar insights de persona sint√©tica."

@st.cache_data(show_spinner=True)
def generate_ice_score_tests(analysis_results):
    sentiment = analysis_results.get('sentiment', {})
    topics = analysis_results.get('topics', [])
    term_clusters = analysis_results.get('term_clusters', {})
    topic_relations = analysis_results.get('topic_relations', [])
    prompt = f"""
Com base na an√°lise de social listening fornecida, atue como um Growth Hacker experiente. Sugira EXATAMENTE 10 testes de Growth priorizados usando a metodologia ICE Score (Impacto, Confian√ßa, Facilidade).
Para cada teste, identifique UMA √öNICA VARI√ÅVEL de alavancagem principal entre: "Canal", "Segmenta√ß√£o", "Formato", "Criativo" ou "Copy/Argumento".
Apresente os resultados em formato JSON, como uma lista de objetos. N√£o inclua nenhum texto adicional antes ou depois do JSON.
A lista de testes deve ser ordenada do maior para o menor ICE Score.
Informa√ß√µes da an√°lise:
Sentimento Geral: {json.dumps(sentiment, ensure_ascii=False)}
Temas Mais Citados: {json.dumps(topics, ensure_ascii=False)}
Agrupamento de Termos: {json.dumps(term_clusters, ensure_ascii=False)}
Rela√ß√£o entre Temas: {json.dumps(topic_relations, ensure_ascii=False)}
Exemplo:
[
  {{
    "Ordem": 1,
    "Nome do Teste": "Teste de Manchete de An√∫ncio",
    "Descri√ß√£o do Teste": "Testar diferentes manchetes para an√∫ncios no Facebook Ads para aumentar o CTR, focando no tema 'Benef√≠cios Previdenci√°rios'.",
    "Vari√°vel de Alavancagem": "Copy/Argumento",
    "Impacto (1-10)": 9,
    "Confian√ßa (1-10)": 8,
    "Facilidade (1-10)": 7,
    "ICE Score": 8.00
  }}
]
"""
    try:
        response = model.generate_content(prompt)
        response_text = response.text.strip()
        if response_text.startswith("```json"):
            response_text = response_text[len("```json"):].strip()
        if response_text.endswith("```"):
            response_text = response_text[:-len("```")].strip()
        data = json.loads(response_text)
        return data
    except json.JSONDecodeError as e:
        st.error(f"Erro ao gerar ICE Score: {e}")
        st.code(f"Resposta bruta do Gemini (ICE Score): {response_text}")
        return None
    except Exception as e:
        st.error(f"Erro inesperado ao gerar testes de Growth com ICE Score: {e}")
        return None

# --- VISUALIZA√á√ÉO ---
def plot_sentiment_chart(sentiment_data):
    labels_order = ['positive', 'neutral', 'negative', 'no_sentiment_detected']
    display_labels = ['Positivo', 'Neutro', 'Negativo', 'N√£o Detectado']
    colors_for_pie = {
        'positive': '#ff99b0',
        'neutral': '#1f2329',
        'negative': '#fe1874',
        'no_sentiment_detected': '#cccccc'
    }
    sizes = [sentiment_data.get(label, 0.0) for label in labels_order]
    filtered_data = [(display_labels[i], sizes[i], colors_for_pie[labels_order[i]])
                     for i, size in enumerate(sizes) if size > 0]
    if not filtered_data:
        st.warning("Dados insuficientes para gr√°fico de sentimento.")
        return
    filtered_labels, filtered_sizes, filtered_colors = zip(*filtered_data)
    explode = [0.03] * len(filtered_labels)
    fig, ax = plt.subplots(figsize=(6, 6))
    wedges, texts, autotexts = ax.pie(
        filtered_sizes,
        explode=explode,
        labels=filtered_labels,
        colors=filtered_colors,
        autopct='%1.1f%%',
        startangle=90,
        pctdistance=0.85
    )
    for autotext in autotexts:
        autotext.set_color('#f3f3f3')
        autotext.set_fontsize(12)
        autotext.set_fontweight('bold')
    for text in texts:
        text.set_color('#1f2329')
        text.set_fontsize(10)
    centre_circle = plt.Circle((0,0),0.70,fc='#f3f3f3')
    fig.gca().add_artist(centre_circle)
    ax.axis('equal')
    ax.set_title('1. An√°lise de Sentimento Geral', pad=18, color='#1f2329')
    st.pyplot(fig)

def plot_topics_chart(topics_data):
    if not topics_data:
        st.warning("Dados de temas insuficientes.")
        return
    df_topics = pd.DataFrame(topics_data)
    df_topics['positive'] = df_topics['positive'].fillna(0).astype(int)
    df_topics['neutral'] = df_topics['neutral'].fillna(0).astype(int)
    df_topics['negative'] = df_topics['negative'].fillna(0).astype(int)
    df_topics['Total'] = df_topics['positive'] + df_topics['neutral'] + df_topics['negative']
    df_topics = df_topics.sort_values('Total', ascending=True)
    fig, ax = plt.subplots(figsize=(8, max(4, len(df_topics) * 0.5)))
    bar_colors = ['#ff99b0', '#1f2329', '#fe1874']
    df_topics[['positive', 'neutral', 'negative']].plot(
        kind='barh',
        stacked=True,
        color=bar_colors,
        ax=ax
    )
    ax.set_title('2. Temas Mais Citados por Sentimento', color='#1f2329')
    ax.set_xlabel('N√∫mero de Coment√°rios', color='#1f2329')
    ax.set_ylabel('Tema', color='#1f2329')
    ax.set_yticklabels(df_topics['name'], color='#1f2329')
    ax.tick_params(axis='x', colors='#1f2329')
    ax.tick_params(axis='y', colors='#1f2329')
    ax.legend(['Positivo', 'Neutro', 'Negativo'], loc='lower right', frameon=False, labelcolor='#1f2329')
    plt.tight_layout()
    st.pyplot(fig)

def plot_word_cloud(term_clusters_data):
    if not term_clusters_data:
        st.warning("Dados de termos insuficientes para nuvem de palavras.")
        return
    def color_func(word, font_size, position, orientation, random_state=None, **kwargs):
        import random
        return '#fe1874' if random_state and random_state.randint(0, 2) == 0 else '#1f2329'
    wordcloud = WordCloud(
        width=700,
        height=400,
        background_color='#f3f3f3',
        color_func=color_func,
        min_font_size=12,
        max_words=60,
        prefer_horizontal=0.8,
        collocations=False
    ).generate_from_frequencies(term_clusters_data)
    fig = plt.figure(figsize=(8, 5))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis('off')
    plt.title('3. Agrupamento de Termos (Nuvem de Palavras)', pad=16, fontsize=15)
    st.pyplot(fig)

def plot_topic_relations_chart(topic_relations_data):
    if not topic_relations_data:
        st.warning("Dados insuficientes para grafo de rela√ß√£o entre temas.")
        return
    G = nx.Graph()
    for rel in topic_relations_data:
        source = rel.get('source')
        target = rel.get('target')
        description = rel.get('description')
        if source and target:
            G.add_edge(source, target, description=description)
    if not G.edges():
        st.warning("Nenhuma rela√ß√£o v√°lida encontrada para construir o grafo de rede.")
        return
    fig, ax = plt.subplots(figsize=(8, 7))
    pos = nx.spring_layout(G, k=0.7, iterations=50, seed=42)
    node_colors = ['#fe1874' for _ in G.nodes()]
    nx.draw_networkx_nodes(G, pos, node_size=2000, node_color=node_colors, alpha=0.9, ax=ax)
    nx.draw_networkx_edges(G, pos, width=1.2, edge_color='#1f2329', alpha=0.6, ax=ax)
    nx.draw_networkx_labels(G, pos, font_size=8, font_weight='bold', font_color='#1f2329', ax=ax)
    ax.set_title('4. Rela√ß√£o Entre Temas (Grafo de Rede)', pad=16, color='#1f2329')
    plt.axis('off')
    plt.tight_layout()
    st.pyplot(fig)

# --- APP STREAMLIT ---
st.set_page_config(layout="wide", page_title="Social Listening Tool + AI")
st.title("üó£Ô∏è Social Listening Tool + AI")
st.markdown("---")

st.markdown(f"Carregue uma base de coment√°rios (.csv, .xls, .xlsx, .doc, .docx), uma URL de v√≠deo do YouTube, ou cole coment√°rios no campo abaixo. **Ser√£o processados no m√°ximo {MAX_COMMENTS_TO_PROCESS} coment√°rios para an√°lise pela IA.**")

all_comments_list = []
original_comment_count = 0 # Vari√°vel para armazenar a contagem original antes do corte

col1, col2 = st.columns(2)
with col1:
    uploaded_file = st.file_uploader(
        "Fa√ßa upload do arquivo de coment√°rios (.csv, .xls, .xlsx, .doc, .docx):",
        type=["csv", "xls", "xlsx", "doc", "docx"],
        key="fileuploader"
    )
    if uploaded_file is not None:
        file_extension = os.path.splitext(uploaded_file.name)[1].lower()
        file_contents = uploaded_file.read()
        extracted_comments = extract_text_from_file(file_contents, file_extension)
        original_comment_count = len(extracted_comments) # Armazena a contagem antes do corte
        all_comments_list = extracted_comments[:MAX_COMMENTS_TO_PROCESS]

with col2:
    youtube_url_input = st.text_input("Ou insira uma URL de v√≠deo do YouTube:")
    if youtube_url_input:
        yt_comments = download_youtube_comments(youtube_url_input.strip())
        if yt_comments:
            original_comment_count = len(yt_comments) # Armazena a contagem antes do corte
            all_comments_list = yt_comments[:MAX_COMMENTS_TO_PROCESS]

manual_text = st.text_area("Ou cole coment√°rios (um por linha):")
if manual_text and not all_comments_list:
    manual_comments = [l for l in manual_text.split("\n") if l.strip()]
    if manual_comments:
        original_comment_count = len(manual_comments) # Armazena a contagem antes do corte
        all_comments_list = manual_comments[:MAX_COMMENTS_TO_PROCESS]
        st.success(f"{len(all_comments_list)} coment√°rios colados e prontos para an√°lise.")

if all_comments_list:
    # Mensagem na tarja verde ajustada
    if original_comment_count > MAX_COMMENTS_TO_PROCESS:
        st.success(f"Coment√°rios carregados! O limite de {MAX_COMMENTS_TO_PROCESS} coment√°rios processados nessa vers√£o teste da ferramenta foi atingido. Esse limite √© importante para garantir que todos os usu√°rios tenham acesso a um ambiente de teste est√°vel.")
        st.info(f"Ser√£o processados os primeiros {len(all_comments_list)} coment√°rios para an√°lise.")
    else:
        st.success(f"Coment√°rios carregados! Ser√£o processados {len(all_comments_list)} coment√°rios para an√°lise.")

    text_to_analyze = "\n".join(all_comments_list)
    with st.spinner("Processando an√°lise com Gemini..."):
        analysis_results = analyze_text_with_gemini(text_to_analyze)
    if analysis_results:
        tabs = st.tabs([
            "üìä Sentimento",
            "üí° Temas",
            "üîë Termos-Chave",
            "üîó Rela√ß√µes entre Temas",
            "üìù An√°lise Qualitativa",
            "üßë‚Äçüíº Persona Sint√©tica",
            "üöÄ Testes de Growth (ICE Score)"
        ])
        with tabs[0]:
            st.subheader("Sentimento Geral")
            plot_sentiment_chart(analysis_results.get('sentiment', {}))
            with st.expander("Ver dados brutos de sentimento"):
                st.json(analysis_results.get('sentiment', {}))
        with tabs[1]:
            st.subheader("Temas mais Citados com Sentimento")
            plot_topics_chart(analysis_results.get('topics', []))
            with st.expander("Ver dados brutos dos temas"):
                st.json(analysis_results.get('topics', []))
        with tabs[2]:
            st.subheader("Agrupamento de Termos/Nuvem de Palavras")
            plot_word_cloud(analysis_results.get('term_clusters', {}))
            with st.expander("Ver dados brutos de termos-chave"):
                st.json(analysis_results.get('term_clusters', {}))
        with tabs[3]:
            st.subheader("Rela√ß√£o entre Temas (Grafo de Rede)")
            plot_topic_relations_chart(analysis_results.get('topic_relations', []))
            with st.expander("Ver dados brutos das rela√ß√µes entre temas"):
                st.json(analysis_results.get('topic_relations', []))
        with tabs[4]:
            st.subheader("An√°lise Qualitativa")
            with st.spinner("Gerando an√°lise qualitativa..."):
                qualitative = generate_qualitative_analysis(analysis_results, text_to_analyze)
                st.markdown(qualitative)
        with tabs[5]:
            st.subheader("Persona Sint√©tica (Insights)")
            with st.spinner("Gerando insights de persona..."):
                persona = generate_persona_insights(analysis_results, text_to_analyze)
                st.markdown(persona)
        with tabs[6]:
            st.subheader("Sugest√µes de Testes de Growth (ICE Score)")
            with st.spinner("Gerando sugest√µes de testes de growth..."):
                ice = generate_ice_score_tests(analysis_results)
                if ice:
                    df_ice = pd.DataFrame(ice)
                    cols = ["Ordem", "Nome do Teste", "Descri√ß√£o do Teste", "Vari√°vel de Alavancagem", "Impacto (1-10)", "Confian√ßa (1-10)", "Facilidade (1-10)", "ICE Score"]
                    df_ice = df_ice[[c for c in cols if c in df_ice.columns]]
                    df_ice = df_ice.sort_values(by="ICE Score", ascending=False)
                    st.dataframe(df_ice, hide_index=True, use_container_width=True)
                    with st.expander("Ver dados brutos dos testes de growth (ICE Score)"):
                        st.json(ice)
                else:
                    st.warning("N√£o foi poss√≠vel gerar sugest√µes de testes de growth.")
    else:
        st.error("N√£o foi poss√≠vel gerar a an√°lise com Gemini. Reveja os dados e tente novamente.")
else:
    st.info("Fa√ßa o upload de coment√°rios, cole manualmente ou insira uma URL do YouTube para iniciar a an√°lise.")

# --- FOOTER / SE√á√ÉO DE CAPTA√á√ÉO DE E-MAIL COM TALLY ---
st.markdown("---") # Linha divis√≥ria para separar do conte√∫do principal

st.subheader("üí° Gostou de testar a aplica√ß√£o?")
st.markdown("""
    Essa vers√£o de teste possu√≠ uma limita√ß√£o de coment√°rios que podem ser analisados e de volume de an√°lises por dia.

    Caso tenha interesse em acessar a aplica√ß√£o completa, sinalize por aqui, e eu te avisarei sobre o lan√ßamento da vers√£o final da aplica√ß√£o com acesso irrestrito.
""")

# Link para o formul√°rio Tally
# Use o f-string para facilitar a inclus√£o do link.
TALLY_FORM_URL = "https://tally.so/r/w2yrzg"
st.markdown(f"[**Clique aqui!**]({TALLY_FORM_URL})")

st.markdown("---") # Outra linha divis√≥ria no final

st.markdown("Desenvolvido com Python, ‚ù§Ô∏è e AI por Pedro Costa | Product Marketing & Martech Specialist")
